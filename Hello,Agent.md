# Hello,Agent!



## 第一章：初识智能体

在人工智能领域，智能体被定义为任何能够通过**传感器（Sensors）**感知其所处**环境（Environment）**，并**自主**地通过**执行器（Actuators）**采取**行动（Action）**以达成特定目标的实体。

### 1.1历史的智能体

#### 1.1.1智能体演进历史

智能体演进的起点，是结构最简单的反射智能体。他们是由工程师明确设计的“条件-动作”规则构成：例如经典的自动恒温器，若传感器感知的室温高于设定值，则启动制冷系统。

Q1：但是如果环境的当前状态不足以作为决策的全部依据，智能体该怎么办？![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-1.png)

A1：为了解决此问题，研究者引入了“状态”的概念，发展出**基于模型的反射智能体（Model-Based Reflex Agent）**。

​	这类智能体拥有一个内部的**世界模型**，用于追踪和理解环境中那些无法被直接感知的方面。

​	【例如，一辆在隧道中行驶的自动驾驶汽车，即便摄像头暂时无法感知到前方的车辆，它的内部模型依然会维持对那辆车存在、速度和预估位置的判断。】

Q2：但是只有理解，没有目标，智能体不知道要怎么推进。

A2：这促进了**基于目标的智能体（Goal-Based Agent）**的发展。与前两者不同，它的行为不再是被动地对环境做出反应，而是主动地、有预见性地选择能够导向某个特定未来状态的行动。

​	【经典的例子是 GPS 导航系统：你的目标是到达公司，智能体会基于地图数据（世界模型），通过搜索算法（如 A*算法）来规划（Planning）出一条最优路径。】

Q3：但是目标并不单一，我们不仅要到达公司，还要时间短，省油，避开拥堵路段。

A3：当多个目标需要权衡时，**基于效用的智能体（Utility-Based Agent）**便随之出现。它为每一个可能的世界状态都赋予一个效用值，这个值代表了满意度的高低。智能体的核心目标不再是简单地达成某个特定状态，而是最大化期望效用。

Q4：但是这些依然是依赖于预设，我们想要通过环境自主学习的智能体。

A4：这便是**学习型智能体（Learning Agent）**的核心思想，而**强化学习（Reinforcement Learning, RL）**是实现这一思想最具代表性的路径。

​	一个学习型智能体包含一个性能元件（即我们前面讨论的各类智能体）和一个学习元件。学习元件通过观察性能元件在环境中的行动所带来的结果来不断修正性能元件的决策策略。

------

#### 1.1.2LLM驱动的智能体出现

以GPT为代表的大语言模型的出现，使得学习型智能体的出现成为了可能。

 LLM 智能体则通过在海量数据上的预训练，获得了隐式的世界模型与强大的涌现能力，使其能够以更灵活、更通用的方式应对复杂任务，行为模式不再是工程师既定的。

我们以“***智能体旅行助手***”为例说明这个差异：

​	在 LLM 智能体出现之前，规划旅行通常意味着用户需要在多个专用应用（如天气、地图、预订网站）之间手动切换，并由用户自己扮演信息整合与决策的角色。

​	而一个 LLM 智能体则能将这个流程整合起来。当接收到“规划一次旅行”这样的指令时，它的工作方式体现了以下几点：

- **规划与推理**：智能体首先会将这个高层级目标分解为一系列逻辑子任务，例如：`[确认出行偏好] -> [查询目的地信息] -> [制定行程草案] -> [预订票务住宿]`。这是一个内在的、由模型驱动的规划过程。
- **工具使用**：在执行规划时，智能体识别到信息缺口，会主动调用外部工具来补全。例如，它会调用天气查询接口获取实时天气，并基于“预报有雨”这一信息，在后续规划中倾向于推荐室内活动。
- **动态修正**：在交互过程中，智能体会将用户的反馈（如“这家酒店超出预算”）视为新的约束，并据此调整后续的行动，重新搜索并推荐符合新要求的选项。整个“**查天气 → 调行程 → 订酒店**”的流程，展现了其根据上下文动态修正自身行为的能力。

总而言之，我们正从开发专用自动化工具转向构建能自主解决问题的系统。核心不再是编写代码，而是引导一个通用的“大脑”去规划、行动和学习。

------

#### 1.1.3智能体的分类

对智能体可分为三类：

（1）**基于内部决策架构的分类**

​	第一种分类维度是依据智能体内部决策架构的复杂程度，涵盖了例如：简单的***反应式***智能体，引入内部模型的***模型式***智能体，再到***基于目标***和***基于效用***的智能体...

（2）**基于时间与反应性的分类**

​	可以从智能体处理决策的时间维度进行分类。这个视角关注智能体是在接收到信息后立即行动，还是会经过深思熟虑的规划再行动。这揭示了智能体设计中一个核心权衡：追求速度的**反应性（Reactivity）**与追求最优解的**规划性（Deliberation）**之间的平衡。

- **反应式智能体 (Reactive Agents)**

  这类智能体对环境刺激做出近乎即时的响应，决策延迟极低。例如简单反应式智能体和基于模型智能体。

- **规划式智能体(Deliberative Agents)**

  与反应式智能体相对，规划式（或称审议式）智能体在行动前会进行复杂的思考和规划。它们不会立即对感知做出反应，而是会先利用其内部的世界模型，探索未来的可能性，评估不同行动的后果，以期找到一条能够达成目标的最佳路径 。例如**基于目标**和**基于效用**的智能体是典型的规划式智能体。

- **混合式智能体(Hybrid Agents)**

  现实世界的复杂任务，往往既需要即时反应，也需要长远规划。例如，我们之前提到的智能旅行助手，既要能根据用户的即时反馈（如“这家酒店太贵了”）调整推荐（反应性），又要能规划出为期数天的完整旅行方案（规划性）。因此，混合式智能体旨在结合两者的优点，实现反应与规划的平衡。

**（3）基于知识表示的分类**

- **符号主义 AI（Symbolic AI）**

  其主要优势在于透明和可解释。由于推理步骤明确，其决策过程可以被完整追溯。

- **亚符号主义 AI（连接主义）（Sub-symbolic AI）**

  它能够轻松处理图像、声音等非结构化数据，这在符号主义 AI 看来是极其困难的任务。然而，这种强大的直觉能力也伴随着不透明性。亚符号主义系统通常被视为一个**黑箱**。

- **神经符号主义 AI（Neuro-Symbolic AI）**

  它的目标，是融合两大范式的优点，创造出一个既能像神经网络一样从数据中学习，又能像符号系统一样进行逻辑推理的混合智能体。它试图弥合感知与认知、直觉与理性之间的鸿沟。

​	

​	**大语言模型驱动的智能体**是神经符号主义的一个极佳实践范例。

​	其内核是一个巨大的神经网络，使其具备模式识别和语言生成能力。然而，当它工作时，它会生成一系列结构化的中间步骤，如思想、计划或 API 调用，这些都是明确的、可操作的符号。通过这种方式，它实现了感知与认知、直觉与理性的初步融合。

------

### 1.2智能体的构成与运行原理

#### 1.2.1任务环境

在人工智能领域，通常使用**PEAS 模型**来精确描述一个任务环境，即分析其**性能度量(Performance)、环境(Environment)、执行器(Actuators)和传感器(Sensors)** 。

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-6.png)

1. 首先，环境通常是**部分可观察的**。例如，旅行助手在查询航班时，无法一次性获取所有航空公司的全部实时座位信息。它只能通过调用航班预订 API，看到该 API 返回的部分数据。这就要求智能体必须具备记忆（记住已查询过的航线）和探索（尝试不同的查询日期）的能力。
2. 其次，行动的结果也并非总是确定的。根据结果的可预测性，环境可分为**确定性**和**随机性**。旅行助手的任务环境就是典型的随机性环境。当它搜索票价时，两次相邻的调用返回的机票价格和余票数量都可能不同，这就要求智能体必须具备处理不确定性、监控变化并及时决策的能力。
3. 此外，环境中还可能存在其他行动者，从而形成**多智能体(Multi-agent)** 环境。它们的行动（例如，订走最后一张特价票）会直接改变旅行助手所处环境的状态，这对智能体的快速响应和策略选择提出了更高要求。
4. 最后，几乎所有任务都发生在**序贯**且**动态**的环境中。“序贯”意味着当前动作会影响未来；而“动态”则意味着环境自身可能在智能体决策时发生变化。这就要求智能体的“感知-思考-行动-观察”循环必须能够快速、灵活地适应持续变化的世界。

#### 1.2.2智能体的运行机制

智能体并非一次性完成任务，而是通过一个持续的循环与环境进行交互，这个核心机制被称为 **智能体循环 (Agent Loop)**。

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-5.png)

这个循环主要包含以下几个相互关联的阶段：

1. **感知 (Perception)**：这是循环的起点。智能体通过其传感器（例如，API 的监听端口、用户输入接口）接收来自环境的输入信息。这些信息，即是**观察 (Observation)**，既可以是用户的初始指令，也可以是上一步行动所导致的环境状态变化反馈。
2. **思考 (Thought)**：智能体的***核心决策阶段***。对于 LLM 智能体而言，这通常是由**大语言模型**驱动的内部推理过程。“思考”阶段可进一步细分为两个关键环节：
   - **规划 (Planning)**：智能体基于观察和记忆，更新对任务和环境的理解，并制定或调整一个行动计划。这可能涉及将复杂目标分解为一系列更具体的子任务。
   - **工具选择 (Tool Selection)**：根据当前计划，智能体从其可用的工具库中，选择最适合执行下一步骤的工具，并确定调用该工具所需的具体参数。
3. **行动 (Action)**：决策完成后，智能体通过其执行器（Actuators）执行具体的行动。这通常表现为调用一个选定的工具（如代码解释器、搜索引擎 API），从而对环境施加影响，意图改变环境的状态。

行动不是循环的终点。智能体的行动会引起**环境 (Environment)** 的**状态变化 (State Change)**，环境随即会产生一个新的**观察 (Observation)** 作为结果反馈。这个新的观察又会在下一轮循环中被智能体的感知系统捕获，形成一个持续的“感知-思考-行动-观察”的闭环。智能体正是通过不断重复这一循环，逐步推进任务，从初始状态向目标状态演进。

#### 1.2.3智能体的交互协议

在工程实践中，为了让 LLM 能够有效驱动这个循环，我们需要一套明确的**交互协议 (Interaction Protocol)** 来规范其与环境之间的信息交换。，这一协议体现在对智能体每一次输出的结构化定义上。智能体的输出是一段遵循特定格式的文本，其中明确地展示了其内部的推理过程与最终决策。

这里的交互范式是`Thought-Action-Observation` 交互范式。

这个结构通常包含两个核心部分：

- **Thought (思考)**：它以自然语言形式阐述了智能体如何分析当前情境、回顾上一步的观察结果、进行自我反思与问题分解，并最终规划出下一步的具体行动。
- **Action (行动)**：这是智能体基于思考后，决定对环境施加的具体操作，通常以函数调用的形式表示。

```Bash
Thought: 用户想知道北京的天气。我需要调用天气查询工具。
Action: get_weather("北京")
Observation: 北京当前天气为晴，气温25摄氏度，微风。
```

1. 这里的`Action`字段构成了对外部世界的指令。一个外部的**解析器 (Parser)** 会捕捉到这个指令，并调用相应的`get_weather`函数。
2. 行动执行后，环境会返回一个结果。感知系统的一个重要职责就是扮演传感器的角色：将这个原始JSON输出处理并封装成一段简洁、清晰的自然语言文本，即观察。
3. 这段`Observation`文本会被反馈给智能体，作为下一轮循环的主要输入信息，供其进行新一轮的`Thought`和`Action`。

------

### 1.3实现智能体

前置知识：

`requests`是 Python 社区中最流行、最易用的选择。

`tavily`是一个强大的 AI 搜索 API 客户端，是专供AI用于获取最新实时的网络搜索结果，且返回的JSON格式有利于AI进行获取解析。

`openai`是 OpenAI 官方提供的 Python SDK，用于调用 GPT 等大语言模型服务。

需准备：

（1）指令模板

```
AGENT_SYSTEM_PROMPT = """
你是一个智能旅行助手。你的任务是分析用户的请求，并使用可用工具一步步地解决问题。

# 可用工具:
- `get_weather(city: str)`: 查询指定城市的实时天气。
- `get_attraction(city: str, weather: str)`: 根据城市和天气搜索推荐的旅游景点。

# 输出格式要求:
你的每次回复必须严格遵循以下格式，包含一对Thought和Action：

Thought: [你的思考过程和下一步计划]
Action: [你要执行的具体行动]

Action的格式必须是以下之一：
1. 调用工具：function_name(arg_name="arg_value")
2. 结束任务：Finish[最终答案]

# 重要提示:
- 每次只输出一对Thought-Action
- Action必须在同一行，不要换行
- 当收集到足够信息可以回答用户问题时，必须使用 Action: Finish[最终答案] 格式结束

请开始吧！
"""
```

（2）执行任务所用的工具（即各类功能的API接口）

工具 1：查询真实天气

我们将使用免费的天气查询服务 `wttr.in`，它能以 JSON 格式返回指定城市的天气数据。

工具 2：搜索并推荐旅游景点

我们将定义一个新工具 `search_attraction`，它会根据城市和天气状况，互联网上搜索合适的景点。

最后，我们将所有工具函数放入一个字典，供主循环调用：

```python
# 将所有工具函数放入一个字典，方便后续调用
available_tools = {
    "get_weather": get_weather,
    "get_attraction": get_attraction,
}
```

（3）接入LLM大语言模型

```python
from openai import OpenAI

class OpenAICompatibleClient:
    """
    一个用于调用任何兼容OpenAI接口的LLM服务的客户端。
    """
    def __init__(self, model: str, api_key: str, base_url: str):
        self.model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def generate(self, prompt: str, system_prompt: str) -> str:
        """调用LLM API来生成回应。"""
        print("正在调用大语言模型...")
        try:
            messages = [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': prompt}
            ]
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=False
            )
            answer = response.choices[0].message.content
            print("大语言模型响应成功。")
            return answer
        except Exception as e:
            print(f"调用LLM API时发生错误: {e}")
            return "错误:调用语言模型服务时出错。"
```

（4）实现循环主体

```python
import re

# --- 1. 配置LLM客户端 ---
# 请根据您使用的服务，将这里替换成对应的凭证和地址
API_KEY = "YOUR_API_KEY"
BASE_URL = "YOUR_BASE_URL"
MODEL_ID = "YOUR_MODEL_ID"
TAVILY_API_KEY="YOUR_Tavily_KEY"
os.environ['TAVILY_API_KEY'] = "YOUR_TAVILY_API_KEY"

llm = OpenAICompatibleClient(
    model=MODEL_ID,
    api_key=API_KEY,
    base_url=BASE_URL
)

# --- 2. 初始化 ---
user_prompt = "你好，请帮我查询一下今天北京的天气，然后根据天气推荐一个合适的旅游景点。"
prompt_history = [f"用户请求: {user_prompt}"]

print(f"用户输入: {user_prompt}\n" + "="*40)

# --- 3. 运行主循环 ---
for i in range(5): # 设置最大循环次数
    print(f"--- 循环 {i+1} ---\n")
    
    # 3.1. 构建Prompt
    full_prompt = "\n".join(prompt_history)
    
    # 3.2. 调用LLM进行思考
    llm_output = llm.generate(full_prompt, system_prompt=AGENT_SYSTEM_PROMPT)
    # 模型可能会输出多余的Thought-Action，需要截断
    match = re.search(r'(Thought:.*?Action:.*?)(?=\n\s*(?:Thought:|Action:|Observation:)|\Z)', llm_output, re.DOTALL)
    if match:
        truncated = match.group(1).strip()
        if truncated != llm_output.strip():
            llm_output = truncated
            print("已截断多余的 Thought-Action 对")
    print(f"模型输出:\n{llm_output}\n")
    prompt_history.append(llm_output)
    
    # 3.3. 解析并执行行动
    action_match = re.search(r"Action: (.*)", llm_output, re.DOTALL)
    if not action_match:
        observation = "错误: 未能解析到 Action 字段。请确保你的回复严格遵循 'Thought: ... Action: ...' 的格式。"
        observation_str = f"Observation: {observation}"
        print(f"{observation_str}\n" + "="*40)
        prompt_history.append(observation_str)
        continue
    action_str = action_match.group(1).strip()

    if action_str.startswith("Finish"):
        final_answer = re.match(r"Finish\[(.*)\]", action_str).group(1)
        print(f"任务完成，最终答案: {final_answer}")
        break
    
    tool_name = re.search(r"(\w+)\(", action_str).group(1)
    args_str = re.search(r"\((.*)\)", action_str).group(1)
    kwargs = dict(re.findall(r'(\w+)="([^"]*)"', args_str))

    if tool_name in available_tools:
        observation = available_tools[tool_name](**kwargs)
    else:
        observation = f"错误:未定义的工具 '{tool_name}'"

    # 3.4. 记录观察结果
    observation_str = f"Observation: {observation}"
    print(f"{observation_str}\n" + "="*40)
    prompt_history.append(observation_str)
```

输出如下：

```bash
用户输入: 你好，请帮我查询一下今天北京的天气，然后根据天气推荐一个合适的旅游景点。
========================================
--- 循环 1 ---

正在调用大语言模型...
大语言模型响应成功。
模型输出:
Thought: 首先需要获取北京今天的天气情况，之后再根据天气情况来推荐旅游景点。
Action: get_weather(city="北京")

Observation: 北京当前天气:Sunny，气温26摄氏度
========================================      
--- 循环 2 ---

正在调用大语言模型...
大语言模型响应成功。
模型输出:
Thought: 现在已经知道了北京今天的天气是晴朗且温度适中，接下来可以基于这个信息来推荐一个适合的旅游景点了。
Action: get_attraction(city="北京", weather="Sunny")

Observation: 北京在晴天最值得去的旅游景点是颐和园，因其美丽的湖景和古建筑。另一个推荐是长城，因其壮观的景观和历史意义。
========================================
--- 循环 3 ---

正在调用大语言模型...
大语言模型响应成功。
模型输出:
Thought: 已经获得了两个适合晴天游览的景点建议，现在可以根据这些信息给用户提供满意的答复。
Action: Finish[今天北京的天气是晴朗的，气温26摄氏度，非常适合外出游玩。我推荐您去颐和园欣赏美丽的湖景和古建筑，或者前往长城体验其壮观的景观和深厚的历史意义。希望您有一个愉快的旅行！]

任务完成，最终答案: 今天北京的天气是晴朗的，气温26摄氏度，非常适合外出游玩。我推荐您去颐和园欣赏美丽的湖景和古建筑，或者前往长城体验其壮观的景观和深厚的历史意义。希望您有一个愉快的旅行！
```

这个简单的旅行助手案例，集中演示了基于`Thought-Action-Observation`范式的智能体所具备的四项基本能力：任务分解、工具调用、上下文理解和结果合成。正是通过这个循环的不断迭代，智能体才得以将一个模糊的用户意图，转化为一系列具体、可执行的步骤，并最终达成目标。

### 1.4智能体应用的协作模式

基于智能体在任务中的角色和自主性程度，其协作模式主要分为两种：

1. 一种是作为高效工具，深度融入我们的工作流；
2. 一种则是作为自主的协作者，与其他智能体协作完成复杂目标。

- 作为开发者工具的智能体。

  在这种模式下，智能体被深度集成到开发者的工作流中，作为一种强大的辅助工具。它增强而非取代开发者的角色，通过自动化处理繁琐、重复的任务，让开发者能更专注于创造性的核心工作。比如Trae等...

- 作为自主协作者的智能体

  在这种模式下，我们不再是手把手地指导 AI 完成每一步，而是将一个高层级的目标委托给它。智能体会像一个真正的项目成员一样，独立地进行规划、推理、执行和反思，直到最终交付成果。

#### 1.4.2 Workflow（工作流）和Agent的差异

简单来说，**Workflow 是让 AI 按部就班地执行指令，而 Agent 则是赋予 AI 自由度去自主达成目标。**

- 工作流是一种传统的自动化范式，其核心是**对一系列任务或步骤进行预先定义的、结构化的编排**。它本质上是一个精确的、静态的流程图，规定了在何种条件下、以何种顺序执行哪些操作。
- 基于大型语言模型的智能体是一个**具备自主性的、以目标为导向的系统**。它不仅仅是执行预设指令，而是能够在一定程度上理解环境、进行推理、制定计划，并动态地采取行动以达成最终目标。



## 第二章：智能体发展史

AI智能体演进历史

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/2-figures/1757246501849-00.png)

**每一个新范式的出现，都是为了解决上一代范式的核心“痛点”或根本局限。** 而新的解决方案在带来能力飞跃的同时，也引入了新的、在当时难以克服的“局限”，而这又为下一代范式的诞生埋下了伏笔。理解这一“问题驱动”的迭代历程，能帮助我们更深刻地把握现代智能体技术选型背后的深层原因与历史必然性。



### 2.1基于符号与逻辑的早期智能体

人工智能领域的早期探索，深受数理逻辑和计算机科学基本原理的影响。在那个时代，研究者们普遍持有一种信念：人类的智能，尤其是逻辑推理能力，可以被形式化的符号体系所捕捉和复现。这一核心思想催生了人工智能的第一个重要范式——符号主义（Symbolicism），也被称为“逻辑AI”或“传统AI”。

#### 2.1.1物理符号系统假说

物理符号假说(PSSH)宣称:智能的本质,就是符号的计算与处理.

这个假说具有深远的影响。它将对人类心智这一模糊、复杂的哲学问题的研究，转化为了一个可以在计算机上进行工程化实现的具体问题。

#### 2.1.2专家系统

​	**专家系统（Expert System）**是符号主义时代最重要、最成功的应用成果。专家系统的核心目标，是模拟人类专家在特定领域内解决问题的能力。它通过将专家的知识和经验编码成计算机程序，使其能够在面对相似问题时，给出媲美甚至超越人类专家的结论或建议。

专家系统的“智能”主要源于其两大核心组件：**知识库**和**推理机**。

- **知识库（Knowledge Base）**：这是专家系统的知识存储中心，用于存放领域专家的知识和经验。(常用的一种知识表示方法是**产生式规则（Production Rules）**，即一系列“IF-THEN”形式的条件语句)

- **推理机（Inference Engine）**：推理机是专家系统的核心计算引擎。它是一个通用的程序，其任务是根据用户提供的事实，在知识库中寻找并应用相关的规则，从而推导出新的结论。

  ​	推理机的工作方式主要有两种：

  - **正向链（Forward Chaining）**：从已知事实出发，不断匹配规则的IF部分，触发THEN部分的结论，并将新结论加入事实库，直到最终推导出目标或无新规则可匹配。
  - **反向链（Backward Chaining）**：从一个假设的目标（比如“病人是否患有肺炎”）出发，寻找能够推导出该目标的规则，然后将该规则的IF部分作为新的子目标，如此递归下去，直到所有子目标都能被已知事实所证明。

#### 2.1.3符号主义的瓶颈

​	符号主义AI在从“微观世界”走向开放、复杂的现实世界时，遇到了其方法论固有的根本性难题。这些难题主要可归结为两大类：

**（1）常识知识与知识获取瓶颈**

符号主义智能体的“智能”完全依赖于其知识库的质量和完备性。然而，如何构建一个能够支撑真实世界交互的知识库，是一项极其艰巨的任务，主要体现在两个方面：

- **知识获取瓶颈（Knowledge Acquisition Bottleneck）**：

  专家系统的知识需要由人类专家和知识工程师通过繁琐的访谈、提炼和编码过程来构建。这个过程成本高昂、耗时漫长，且难以规模化。

  更重要的是，人类专家的许多知识是内隐的、直觉性的，很难被清晰地表达为“IF-THEN”规则。试图将整个世界的知识都进行手工符号化，被认为是一项几乎不可能完成的任务。

- **常识问题（Common-sense Problem）**：人类行为依赖于庞大的常识背景（例如，“水是湿的”、“绳子可以拉不能推”），但符号系统除非被明确编码，否则对此一无所知。

  **（2）框架问题与系统脆弱性**

  除了知识层面的挑战，符号主义在处理动态变化的世界时也遇到了逻辑上的困境。

  - **框架问题（Frame Problem）**：在一个动态世界中，智能体执行一个动作后，如何高效判断哪些事物未发生改变是一个逻辑难题。为每个动作显式地声明所有不变的状态，在计算上是不可行的，而人类却能毫不费力地忽略不相关的变化。
  - **系统脆弱性（Brittleness）**：符号系统完全依赖预设规则，导致其行为非常“脆弱”。一旦遇到规则之外的任何微小变化或新情况，系统便可能完全失灵.

### 2.2基于规则的聊天机器人

​	本节我们将通过一个具体的编程实践，来直观地感受基于规则的系统是如何工作的。我们将复现人工智能历史上一个极具影响力的早期聊天机器人——ELIZA。

#### 2.2.1ELIZA的设计思想

它从不正面回答问题或提供信息，而是通过识别用户输入中的关键词，然后应用一套预设的转换规则，将用户的陈述转化为一个开放式的提问。例如，当用户说“我为我的男朋友感到难过”时，ELIZA可能会识别出关键词“我为……感到难过”，并应用规则生成回应：“你为什么会为你的男朋友感到难过？”

#### 2.2.2模式匹配与文本转换

ELIZA的算法流程基于**模式匹配（Pattern Matching）与文本替换（Text Substitution）**，可被清晰地分解为以下四个步骤：

1. **关键词识别与排序：**规则库为每个关键词（如 `mother`, `dreamed`, `depressed`）设定一个优先级。当输入包含多个关键词时，程序会选择优先级最高的关键词所对应的规则进行处理。

2. **分解规则**：

   找到关键词后，程序使用带通配符（*）的分解规则来捕获句子的其余部分。

   1. **规则示例**： `* my *`
   2. **用户输入**： `"My mother is afraid of me"`
   3. **捕获结果**： `["", "mother is afraid of me"]`

3. **重组规则**：

   程序从与分解规则关联的一组重组规则中，选择一条来生成回应（通常随机选择以增加多样性），并可选择性地使用上一步捕获的内容。

   1. **规则示例**： `"Tell me more about your family."`
   2. **生成输出**： `"Tell me more about your family."`

4. **代词转换：**在重组前，程序会进行简单的代词转换（如 `I` → `you`, `my` → `your`），以维持对话的连贯性。

#### 2.2.3 核心逻辑

迷你版ELIZA的实现代码,用于展示核心工作机制:

```python
import re
import random

# 定义规则库:模式(正则表达式) -> 响应模板列表
rules = {
    r'I need (.*)': [
        "Why do you need {0}?",
        "Would it really help you to get {0}?",
        "Are you sure you need {0}?"
    ],
    r'Why don\'t you (.*)\?': [
        "Do you really think I don't {0}?",
        "Perhaps eventually I will {0}.",
        "Do you really want me to {0}?"
    ],
    r'Why can\'t I (.*)\?': [
        "Do you think you should be able to {0}?",
        "If you could {0}, what would you do?",
        "I don't know -- why can't you {0}?"
    ],
    r'I am (.*)': [
        "Did you come to me because you are {0}?",
        "How long have you been {0}?",
        "How do you feel about being {0}?"
    ],
    r'.* mother .*': [
        "Tell me more about your mother.",
        "What was your relationship with your mother like?",
        "How do you feel about your mother?"
    ],
    r'.* father .*': [
        "Tell me more about your father.",
        "How did your father make you feel?",
        "What has your father taught you?"
    ],
    r'.*': [
        "Please tell me more.",
        "Let's change focus a bit... Tell me about your family.",
        "Can you elaborate on that?"
    ]
}

# 定义代词转换规则
pronoun_swap = {
    "i": "you", "you": "i", "me": "you", "my": "your",
    "am": "are", "are": "am", "was": "were", "i'd": "you would",
    "i've": "you have", "i'll": "you will", "yours": "mine",
    "mine": "yours"
}

def swap_pronouns(phrase):
    """
    对输入短语中的代词进行第一/第二人称转换
    """
    words = phrase.lower().split()
    swapped_words = [pronoun_swap.get(word, word) for word in words]
    return " ".join(swapped_words)

def respond(user_input):
    """
    根据规则库生成响应
    """
    for pattern, responses in rules.items():
        match = re.search(pattern, user_input, re.IGNORECASE)
        if match:
            # 捕获匹配到的部分
            captured_group = match.group(1) if match.groups() else ''
            # 进行代词转换
            swapped_group = swap_pronouns(captured_group)
            # 从模板中随机选择一个并格式化
            response = random.choice(responses).format(swapped_group)
            return response
    # 如果没有匹配任何特定规则，使用最后的通配符规则
    return random.choice(rules[r'.*'])

# 主聊天循环
if __name__ == '__main__':
    print("Therapist: Hello! How can I help you today?")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Therapist: Goodbye. It was nice talking to you.")
            break
        response = respond(user_input)
        print(f"Therapist: {response}")
        
>>>
Therapist: Hello! How can I help you today?
You: I am feeling sad today.
Therapist: How long have you been feeling sad today?
You: I need some help with my project.
Therapist: Are you sure you need some help with your project?
You: My mother is not happy with my work.
Therapist: Tell me more about your mother.
You: quit
Therapist: Goodbye. It was nice talking to you.

```

从个人实验中很容易看出其中的缺点:

- **缺乏语义理解**：系统不理解词义。例如，面对“I am **not** happy”的输入，它仍会机械地匹配 `I am (.*)` 规则并生成语义不通的回应，因为它无法理解否定词“not”的作用。
- **无上下文记忆**：系统是**无状态的（Stateless）**，每次回应仅基于当前单句输入，无法进行连贯的多轮对话。
- **规则的扩展性问题**：尝试增加更多规则会导致规则库的规模爆炸式增长，规则间的冲突与优先级管理将变得极其复杂，最终导致系统难以维护。
- 而且回复均为疑问句,其实并没有很好的解决用户的诉求

### 2.3心智社会

​	符号主义的探索和ELIZA的实践，共同指向了一个问题：通过预设规则构建的、单一的、集中的推理引擎，似乎难以通向真正的智能。无论规则库多么庞大，系统在面对真实世界的模糊性、复杂性和无穷变化时，总是显得僵化而脆弱。

​	20世纪70至80年代，符号主义的局限性日益明显。专家系统虽然在高度垂直的领域取得了成功，但它们无法拥有儿童般的常识；SHRDLU虽然能在一个封闭的积木世界中表现出色，但它无法理解这个世界之外的任何事情；ELIZA虽然能模仿对话，但它对对话内容本身一无所知。这些系统都遵循着一种自上而下（Top-down）的设计思路：一个全知全能的中央处理器，根据一套统一的逻辑规则来处理信息和做出决策,没有自己的主观思想。

​	思考者开始在想:什么是理解,什么是常识,智能体应该如何构建?

​	正是基于这样的反思，明斯基提出了一个颠覆性的构想，他不再将心智视为一个金字塔式的层级结构，而是将其看作一个扁平化的、充满了互动与协作的“社会”。

#### 2.3.1协作的智能体

​	在明斯基的理论框架中，这里的智能体指的是一个极其简单的、专门化的心智过程，它自身是“无心”的。例如，一个负责识别线条的`LINE-FINDER`智能体，或一个负责抓握的`GRASP`智能体。

​	这些简单的智能体被组织起来，形成功能更强大的**机构（Agency）**。一个机构是一组协同工作的智能体，旨在完成一个更复杂的任务。

​	**涌现（Emergence）**是理解心智社会理论的关键:复杂的、有目的性的智能行为，并非由某个高级智能体预先规划，而是从大量简单的底层智能体之间的局部交互中自发产生的。

​	让我们以经典的“搭建积木塔”任务为例，来说明这一过程，如图2.6所示。当一个高层目标（如“我要搭一个塔”）出现时，它会激活一个名为`BUILD-TOWER`的高层机构。

1. `BUILD-TOWER`机构并不知道如何执行具体的物理动作，它的唯一作用是激活它的下属机构，比如`BUILDER`。
2. `BUILDER`机构同样很简单，它可能只包含一个循环逻辑：只要塔还没搭完，就激活`ADD-BLOCK`机构。
3. `ADD-BLOCK`机构则负责协调更具体的子任务，它会依次激活`FIND-BLOCK`、`GET-BLOCK`和`PUT-ON-TOP`这三个子机构。
4. 每一个子机构又由更底层的智能体构成。例如，`GET-BLOCK`机构会激活视觉系统中的`SEE-SHAPE`智能体、运动系统中的`REACH`和`GRASP`智能体。

在这个过程中，没有任何一个智能体或机构拥有整个任务的全局规划。当这个由无数“无心”的智能体组成的社会，通过简单的激活和抑制规则相互作用时，一个看似高度智能的行为，搭建积木塔，就自然而然地涌现了出来。

#### 2.3.2对于现代多智能体系统的启发

​	心智社会理论最深远的影响，在于它为**分布式人工智能（Distributed Artificial Intelligence, DAI）**以及后来的**多智能体系统（Multi-Agent System, MAS）**提供了重要的概念基础。

**如果一个心智内部的智能，是通过大量简单智能体的协作而涌现的，那么，在多个独立的、物理上分离的计算实体（计算机、机器人）之间，是否也能通过协作涌现出更强大的“群体智能”？**

心智社会在以下几个方面直接启发了多智能体系统的研究：

- **去中心化控制（Decentralized Control）**：理论的核心在于不存在中央控制器。
- **涌现式计算（Emergent Computation）**：复杂问题的解决方案可以从简单的局部交互规则中自发产生。这启发了MAS中大量基于涌现思想的算法，如蚁群算法、粒子群优化等，用于解决复杂的优化和搜索问题。
- **智能体的社会性（Agent Sociality）**：明斯基的理论强调了智能体之间的交互（激活、抑制）。即系统地研究智能体之间的通信语言（如ACL）、交互协议（如契约网）、协商策略、信任模型乃至组织结构，从而构建起真正的计算社会。

### 2.4范式的演进与现代智能体

​	“心智社会”理论与符号主义在应对真实世界复杂性时暴露的挑战共同指向了一个问题：如果智能无法被完全设计，那么它是否可以被学习出来？

​	这一设问开启了人工智能的“学习”时代。其核心目标不再是手动编码知识，而是构建能从经验和数据中自动获取知识与能力的系统。

#### 2.4.1联结主义

​	与符号主义自上而下、依赖明确逻辑规则的设计哲学不同，联结主义是一种自下而上的方法，其灵感来源于对生物大脑神经网络结构的模仿。它的核心思想可以概括为以下几点：

1. **知识的分布式表示**：知识并非以明确的符号或规则形式存储在某个知识库中，而是以连接权重的形式，分布式地存储在大量简单的处理单元（即人工神经元）的连接之间。整个网络的连接模式本身就构成了知识。
2. **简单的处理单元**：每个神经元只执行非常简单的计算，如接收来自其他神经元的加权输入，通过一个激活函数进行处理，然后将结果输出给下一个神经元。
3. **通过学习调整权重**：系统的智能并非来自于设计者预先编写的复杂程序，而是来自于“学习”过程。系统通过接触大量样本，根据某种学习算法（如反向传播算法）自动、迭代地调整神经元之间的连接权重，从而使得整个网络的输出逐渐接近期望的目标。

在这种范式下，智能体不再是一个被动执行规则的逻辑推理机，而是一个能够通过经验自我优化的适应性系统。

联结主义的兴起，特别是深度学习在21世纪的成功，为智能体赋予了强大的感知和模式识别能力，使其能够直接从原始数据（如图像、声音、文本）中理解世界，这是符号主义时代难以想象的。然而，如何让智能体学会在与环境的动态交互中做出最优的序贯决策，则需要另一种学习范式的补充。

#### 2.4.2基于强化学习的智能体

​	联结主义主要解决了感知问题（例如，“这张图片里有什么？”），但智能体更核心的任务是进行决策（例如，“在这种情况下，我应该做什么？”）。

​	**强化学习（Reinforcement Learning, RL）**正是专注于解决序贯决策问题的学习范式。它并非直接从标注好的静态数据集中学习，而是通过智能体与环境的直接交互，在“试错”中学习如何最大化其长期收益。(正收益给予正向奖励,负收益给予负向奖励)

​	这种通过与环境互动、根据反馈信号来优化自身行为的学习机制，就是强化学习的核心框架。

​	强化学习的框架可以用几个核心要素来描述：

- **智能体（Agent）**：学习者和决策者。在AlphaGo的例子中，就是其决策程序。
- **环境（Environment）**：智能体外部的一切，是智能体与之交互的对象。对AlphaGo而言，就是围棋的规则和对手。
- **状态（State, S）**：对环境在某一时刻的特定描述，是智能体做出决策的依据。例如，棋盘上所有棋子的当前位置。
- **行动（Action, A）**：智能体根据当前状态所能采取的操作。例如，在棋盘的某个合法位置上落下一子。
- **奖励（Reward, R）**：环境在智能体执行一个行动后，反馈给智能体的一个标量信号，用于评价该行动在特定状态下的好坏。例如，在一局棋结束后，胜利获得+1的奖励，失败获得-1的奖励。

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/2-figures/1757246501849-6.png)

这个循环的具体步骤如下：

1. 在时间步t，智能体观察到环境的当前状态St。
2. 基于状态 *St*，智能体根据其内部的**策略（Policy, π）**选择一个行动 At 并执行它。策略本质上是一个从状态到行动的映射，定义了智能体的行为方式。
3. 环境接收到行动 At后，会转移到一个新的状态 St+1。
4. 同时，环境会反馈给智能体一个即时奖励 Rt+1。
5. 智能体利用这个反馈（新状态 St+1 和奖励 Rt+1）来更新和优化其内部策略，以便在未来做出更好的决策。这个更新过程就是学习。

智能体的学习目标，并非最大化某一个时间步的即时奖励，而是最大化从当前时刻开始到未来的**累积奖励（Cumulative Reward）**，也称为**回报（Return）**。这意味着智能体需要具备“远见”，有时为了获得未来更大的奖励，需要牺牲当前的即时奖励。



#### 2.4.3强化学习的预训练

强化学习赋予了智能体从交互中学习决策策略的能力，但这通常需要海量的、针对特定任务的交互数据，导致智能体在学习之初缺乏先验知识，需要从零开始构建对任务的理解。

如何让智能体在开始学习具体任务前，就先具备对世界的广泛理解？这一问题的解决方案，最终在**自然语言处理（Natural Language Processing, NLP）**领域中浮现，其核心便是基于大规模数据的**预训练（Pre-training）**。

**从特定任务到通用模型**

​	在预训练范式出现之前，传统的自然语言处理模型通常是为单一特定任务（如情感分析、机器翻译）在专门标注的中小规模数据集上从零开始独立训练的。这样暴露出各类问题。

​	**预训练与微调**（Pre-training, Fine-tuning）范式的提出彻底改变了这一现状。其核心思想分为两步：

1. **预训练阶段**：首先在一个包含互联网级别海量文本数据的通用语料库上，通过**自监督学习（Self-supervised Learning）**的方式训练一个超大规模的神经网络模型。这个阶段的目标不是完成任何特定任务，而是学习语言本身内在的规律、语法结构、事实知识以及上下文逻辑。最常见的目标是“预测下一个词”。
2. **微调阶段**：完成预训练后，这个模型就已经学习到了和数据集有关的丰富知识。之后，针对特定的下游任务，只需使用少量该任务的标注数据对模型进行微调，即可让模型适应对应任务。

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/2-figures/1757246501849-7.png)

通过在数万亿级别的文本上进行预训练，大型语言模型的神经网络权重实际上已经构建了一个关于世界知识的、高度压缩的隐式模型。当模型的规模（参数量、数据量、计算量）跨越某个阈值后，它们开始展现出未被直接训练的、预料之外的**涌现能力（Emergent Abilities）**，例如：

- **上下文学习（In-context Learning）**：无需调整模型权重，仅在输入中提供**几个示例（Few-shot）**甚至**零个示例（Zero-shot）**，模型就能理解并完成新的任务。
- **思维链（Chain-of-Thought）推理**：通过引导模型在回答复杂问题前，先输出一步步的推理过程，可以显著提升其在逻辑、算术和常识推理任务上的准确性。

至此，智能体发展的历史长河中，几大关键的技术拼图已经悉数登场：符号主义提供了**逻辑推理的框架**，联结主义和强化学习提供了**学习与决策的能力**，而大型语言模型则提供了前所未有的、通过预训练获得的**世界知识和通用推理能力**。

#### 2.4.4基于大语言模型的智能体

随着大型语言模型技术的飞速发展，以LLM为核心的智能体已成为人工智能领域的新范式。

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/2-figures/1757246501849-8.png)

该流程遵循图所示的架构，具体步骤如下：

1. **感知 (Perception)** ：流程始于**感知模块 (Perception Module)**。它通过传感器从**外部环境 (Environment)** 接收原始输入，形成**观察 (Observation)**。这些观察信息（如用户指令、API返回的数据或环境状态的变化）是智能体决策的起点，处理后将被传递给思考阶段。
2. **思考 (Thought)**：这是智能体的认知核心，对应图中的规划模块 (Planning Module)和大型语言模型 (LLM)的协同工作。
   - **规划与分解**：首先，规划模块接收观察信息，进行高级策略制定。它将宏观目标分解为更具体、可执行的步骤。
   - **推理与决策**：随后，作为中枢的**LLM** 接收来自规划模块的指令，并与**记忆模块 (Memory)** 交互以整合历史信息。LLM进行深度推理，最终决策出下一步要执行的具体操作，这通常表现为一个**工具调用 (Tool Call)**。
3. **行动 (Action)** ：决策完成后，便进入行动阶段，由**执行模块 (Execution Module)** 负责。LLM生成的工具调用指令被发送到执行模块。该模块解析指令，从**工具箱 (Tool Use)** 中选择并调用合适的工具来与环境交互或执行任务。这个与环境的实际交互就是智能体的**行动 (Action)**。
4. **观察 (Observation)** 与循环 ：行动会改变环境的状态，并产生结果。
   - 工具执行后会返回一个**工具结果 (Tool Result)** 给LLM，这构成了对行动效果的直接反馈。同时，智能体的行动产生了一个全新的**环境状态**。
   - 这个“工具结果”和“新的环境状态”共同构成了一轮全新的**观察 (Observation)**。这个新的观察会被感知模块再次捕获，同时LLM会根据行动结果**更新记忆 (Memory Update)**，从而启动下一轮“感知-思考-行动”的循环。

#### 2.4.5智能体发展关键节点概览

主要有三大思潮主导着不同时期的研究范式：

1. **符号主义 (Symbolism)** ：以**赫伯特·西蒙 (Herbert A. Simon)** 、**明斯基 (Marvin Minsky)** 等先驱为代表，认为智能的核心在于对符号的操作与逻辑推理。
2. **联结主义 (Connectionism)** ：其灵感源于对大脑神经网络的模拟。尽管早期发展受限，但在**杰弗里·辛顿 (Geoffrey Hinton)** 等研究者的推动下，反向传播算法为神经网络的复苏奠定了基础。最终，随着深度学习时代的到来，这一思想通过卷积神经网络、Transformer等模型成为当前的主流。
3. **行为主义 (Behaviorism)** ：强调智能体通过与环境的互动和试错来学习最优策略，其现代化身为强化学习 。

进入21世纪20年代，这些思想流派以前所未有的方式深度融合。以GPT系列为代表的大语言模型，其本身是联结主义的产物，却成为了执行符号推理、进行工具调用和规划决策的核心“大脑”，形成了神经-符号结合的现代智能体架构。

<img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/2-figures/1757246501849-9.png" alt="图片描述" style="zoom:150%;" />

当代AI agent技术栈预览

![图片描述](https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/2-figures/1757246501849-10.png)
